{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH1EG9tynf_l"
      },
      "source": [
        "# Introduction ‚ú®\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_h4O_ErtSTY"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1TuesF83uT3BoShpMgIW5NN2itlNIwGX-)\n",
        "![](https://drive.google.com/uc?export=view&id=11FoXiDS0XcQG5R9zUh6luvjfJxxQ3vYT)\n",
        "![](https://drive.google.com/uc?export=view&id=145LaNxAZsxPzXoOuFOxJxak_1J90fs5l)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QA4dyyWzDPh"
      },
      "source": [
        "This notebook demonstrates how to train models for the [AIDA-X](https://github.com/AidaDSP/aida-x) plugin. If run inside of Colab, it will automatically use a free Google Cloud GPU.\n",
        "\n",
        "At the end, you'll have a custom-trained model that you can download and play directly on AIDA-X plugin.\\\n",
        "[DEMO VIDEO]() üîäüîäüîä\n",
        "\n",
        "---\n",
        "This notebook is brought to you collaboration between the [MOD Audio](https://mod.audio) and the [AIDA DSP](https://aidadsp.github.io) teams.\\\n",
        "Some of the code and workflow presented here is inspired by the [NAM](https://github.com/sdatkinson/neural-amp-modeler) training [colab](https://colab.research.google.com/github/sdatkinson/neural-amp-modeler/blob/main/bin/train/easy_colab.ipynb?authuser=1#scrollTo=5CQleTk7GJV8) notebook.\n",
        "\n",
        "---  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96GDn8reahv3"
      },
      "source": [
        "## **Instructions** ([step-by-step video](https://www.youtube.com/watch?v=htpK0QLzeKA))\n",
        "Whenever you see `<- RUN CELL (‚ñ∫)`, you need to press the (‚ñ∫) next to it, to run the code that will fulfill that step.  \n",
        "\n",
        "> The steps in this notebook are pretty straightforward:\n",
        "1.   Set-up üëæ\n",
        "2.   Data üìë\n",
        "3.   Model Training üèãÔ∏è‚Äç‚ôÇÔ∏è\n",
        "4.   Model Evaluation üìà (optional)\n",
        "5.   Model Export ‚úÖ\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Set-up üëæ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HQzdKDSc0NId"
      },
      "outputs": [],
      "source": [
        "#@markdown `<- RUN CELL (‚ñ∫)`\n",
        "\n",
        "#@markdown This will check for GPU availability, prepare the code for you, and mount your drive.\n",
        "\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import IPython\n",
        "from time import sleep\n",
        "import librosa\n",
        "\n",
        "print(\"---\")\n",
        "if 'step' in locals():\n",
        "  print(\"Ready! you can now move to step 1: DATA\")\n",
        "else:\n",
        "\n",
        "  print(\"Checking GPU availability...\", end=\" \")\n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU available! \")\n",
        "  else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU unavailable, using CPU instead.\")\n",
        "    print(\"RECOMMENDED: You can enable GPU through \\\"Runtime\\\" -> \\\"Change runtime type\\\" -> \\\"Hardware accelerator:\\\" GPU -> Save\")\n",
        "\n",
        "  !mkdir /content/temp\n",
        "  !touch /content/temp/logs.txt\n",
        "\n",
        "  print(\"Installing dependencies...\")\n",
        "  !pip3 install --disable-pip-version-check --no-cache-dir auraloss==0.4.0 >> /content/temp/logs.txt 2>&1\n",
        "\n",
        "  print(\"Getting the code...\")\n",
        "  !git clone https://github.com/pilali/GuitarAmpModelling.git >> /content/temp/logs.txt 2>&1\n",
        "  assert os.path.exists(\"/content/GuitarAmpModelling\"), f\"Error getting the code!\"\n",
        "\n",
        "  os.chdir('/content/GuitarAmpModelling')\n",
        "  !git checkout aidadsp_devel >> /content/temp/logs.txt 2>&1\n",
        "\n",
        "  print(\"Checking for code updates...\")\n",
        "  !git clone https://github.com/pilali/CoreAudio/ML.git >> /content/temp/logs.txt 2>&1\n",
        "\n",
        "  !export CUBLAS_WORKSPACE_CONFIG=:4096:2\n",
        "\n",
        "\n",
        "  from colab_functions import wav2tensor, extract_best_esr_model, prep_audio, create_csv_nam_v1_1_1\n",
        "  import plotly.graph_objects as go\n",
        "  from CoreAudioML.networks import load_model\n",
        "  import CoreAudioML.miscfuncs as miscfuncs\n",
        "  from google.colab import files\n",
        "  import io\n",
        "  import shutil\n",
        "  os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "  from google.colab import drive\n",
        "  print(\"Mounting google drive...\")\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  step = 0\n",
        "  print()\n",
        "  print(\"Ready! you can now move to step 1: DATA\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnFHaYnUnlux"
      },
      "source": [
        "# 2. The Data (upload + preprocessing) üìë"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CQleTk7GJV8"
      },
      "source": [
        "### Step 2.1: Download the capture signal\n",
        "Download the pre-crafted \"capture signal\" called [input.wav](https://drive.google.com/file/d/1TNpaPPc9tdCu6OA1VETWvufc7wG2nQTJ/view?usp=sharing) from the provided link.\n",
        "\n",
        "### Step 2.2 Reamp your gear\n",
        "Use the downloaded capture signal to reamp the gear that you want to model. Record the output and save it as \"target.wav\".\n",
        "For a detailed demonstration of how to reamp your gear using the capture signal, refer to this [video tutorial](https://youtu.be/lrvuODtk9W0?t=70) starting at 1:10 and ending at 3:44."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ByCjJDMU25jc"
      },
      "outputs": [],
      "source": [
        "#@markdown `<- RUN CELL (‚ñ∫)`\n",
        "\n",
        "#@markdown Step 1.3 upload\n",
        "#@markdown ---\n",
        "#@markdown * In drive, put the 2 audio files with which you would like to train in a single folder.\n",
        "#@markdown  * `input.wav` : contains the reference (dry/DI) sound.\n",
        "#@markdown  * `target.wav` : contains the target (amped/with effects) sound.\n",
        "#@markdown * Use the file browser in the left panel to find a folder with your audio, right-click **\"Copy Path\", paste below**, and run the cell.\n",
        "#@markdown  * ex. `/content/drive/My Drive/training-data-folder`\n",
        "\n",
        "#/content/drive/Shareddrives/AI/Datasets/NeuralModelling/NeuralCoryWong\n",
        "DATA_DIR = '' #@param {type: \"string\"}\n",
        "\n",
        "assert 'step' in locals(), \"Please run the code in the introduction section first!\"\n",
        "print(\"---\")\n",
        "assert DATA_DIR != '', \"Please input a path for your DATA_DIR\"\n",
        "assert os.path.exists(DATA_DIR), f\"Drive Folder Doesn\\'t Exists: {DATA_DIR}\"\n",
        "if not(set([\"input.wav\", \"target.wav\"]) <= set([x.lower() for x in os.listdir(DATA_DIR)])):\n",
        "  assert \"target.wav\" in [x.lower() for x in os.listdir(DATA_DIR)], \"Make sure you have \\\"input.wav\\\" and \\\"target.wav\\\" inside your data folder\"\n",
        "  i_aud, i_sr = librosa.load(\"Data/input.wav\", sr=None, mono=True)\n",
        "  t_aud, t_sr = librosa.load(os.path.join(DATA_DIR, \"target.wav\"), sr=None, mono=True)\n",
        "  assert t_aud.shape[0] == i_aud.shape[0] , \"Make sure you have \\\"input.wav\\\" and \\\"target.wav\\\" inside your data folder\"\n",
        "  shutil.copy(\"Data/input.wav\", os.path.join(DATA_DIR, \"input.wav\"))\n",
        "\n",
        "i_aud, i_sr = librosa.load(os.path.join(DATA_DIR, \"input.wav\"), sr=None, mono=True)\n",
        "t_aud, t_sr = librosa.load(os.path.join(DATA_DIR, \"target.wav\"), sr=None, mono=True)\n",
        "assert t_aud.shape[0]/t_sr - i_aud.shape[0]/i_sr < 3.0 , \"Input and Target audio files are not the same length!\"\n",
        "if t_aud.shape[0] != i_aud.shape[0]:\n",
        "  print(f\"Warning: Input and Target files are not exactly the same length: {i_aud.shape[0]} and {t_aud.shape[0]}\")\n",
        "assert i_sr==t_sr, \"Input and Target audio files are not the same sample rate!\"\n",
        "\n",
        "file_name = os.path.split(DATA_DIR)[-1]\n",
        "create_csv_nam_v1_1_1(os.path.join(DATA_DIR, \"input.wav\")[0] + \".csv\")\n",
        "prep_audio([os.path.join(DATA_DIR, \"input.wav\"),os.path.join(DATA_DIR, \"target.wav\")],\n", file_name=file_name, norm=False, csv_file=False)
        "\n",
        "step = max(step, 1)\n",
        "print()\n",
        "print(\"Data prepared! you can now move to step 2: TRAINING\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NMA3O0FnsKP"
      },
      "source": [
        "# 3. Model Training üèãÔ∏è‚Äç‚ôÇÔ∏è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "h6RdceOeWdZl"
      },
      "outputs": [],
      "source": [
        "#@markdown `<- RUN CELL (‚ñ∫)`\n",
        "\n",
        "#@markdown Training usually takes around 10 minutes,\n",
        "#@markdown but this can change depending on the duration of\n",
        "#@markdown the training data that you provided and the model_type\n",
        "#@markdown you choose.\\\n",
        "#@markdown Note that training doesn't always lead to the same results.\n",
        "#@markdown You may want to run it a couple of times and compare the results.\n",
        "\n",
        "#@markdown Choose the Model type you want to train:\\\n",
        "#@markdown Generally, the heavier the model the more accurate it is, but also the more CPU it consumes.\n",
        "#@markdown Here's a list of approximate CPU consumption of each model type on a [MOD Dwarf](https://mod.audio/dwarf/):\n",
        "#@markdown * Lightest: 25% CPU\n",
        "#@markdown * Light: 30% CPU\n",
        "#@markdown * Standard: 37% CPU\n",
        "#@markdown * Heavy: 46% CPU\n",
        "model_type = \"Standard\" #@param [\"Lightest\", \"Light\", \"Standard\", \"Heavy\"]\n",
        "#@markdown Some training hyper parameters\n",
        "#@markdown (Recommended: ignore and continue with default values):\n",
        "skip_connection = \"OFF\" #@param [\"ON\", \"OFF\"]\n",
        "epochs = 200 #@param {type:\"slider\", min:100, max:2000, step:20}\n",
        "print(\"---\")\n",
        "\n",
        "if model_type == \"Lightest\":\n",
        "  config_file = \"LSTM-8\"\n",
        "elif model_type == \"Light\":\n",
        "  config_file = \"LSTM-12\"\n",
        "elif model_type == \"Standard\":\n",
        "  config_file = \"LSTM-16\"\n",
        "elif model_type == \"Heavy\":\n",
        "  config_file = \"LSTM-20\"\n",
        "\n",
        "if skip_connection == \"ON\":\n",
        "  skip_con = 1\n",
        "else:\n",
        "  skip_con = 0\n",
        "\n",
        "assert 'step' in locals(), \"Please run the code in the introduction section first!\"\n",
        "assert step>=1, \"Please execute the \\\"1.DATA\\\" cell code to prepare the data for the training!\"\n",
        "\n",
        "!python3 dist_model_recnet.py -l \"$config_file\" -fn \"$file_name\" -sc $skip_con -eps $epochs\n",
        "\n",
        "sleep(1)\n",
        "model_dir = f\"/content/GuitarAmpModelling/Results/{file_name}_{config_file}-{skip_con}\"\n",
        "step = max(step, 2)\n",
        "print(\"Training done!\\nESR after training: \", extract_best_esr_model(model_dir)[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxglIXwBn1sj"
      },
      "source": [
        "# 4. Model Evaluation üìà\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QalvHphTImo_"
      },
      "outputs": [],
      "source": [
        "#@markdown `<- RUN CELL (‚ñ∫)`\n",
        "\n",
        "#@markdown Here you can visualize and listen to the output of your trained model on the data you provided earlier.\n",
        "\n",
        "assert 'step' in locals(), \"Please run the code in the introduction section first!\"\n",
        "assert step>=1, \"Please execute the \\\"1.DATA\\\" cell code to prepare the data for the training!\"\n",
        "assert step>=2, \"Please execute the \\\"2.TRAINING\\\" cell code to train a model for evaluation!\"\n",
        "\n",
        "\n",
        "print(\"---\")\n",
        "\n",
        "model_dir = f\"/content/GuitarAmpModelling/Results/{file_name}_{config_file}-{skip_con}\"\n",
        "model_path, esr = extract_best_esr_model(model_dir)\n",
        "model_data = miscfuncs.json_load(model_path)\n",
        "model = load_model(model_data).to(device)\n",
        "model\n",
        "\n",
        "\n",
        "full_dry = wav2tensor(f\"/content/GuitarAmpModelling/Data/test/{file_name}-input.wav\")\n",
        "full_amped = wav2tensor(f\"/content/GuitarAmpModelling/Data/test/{file_name}-target.wav\")\n",
        "\n",
        "\n",
        "samples_viz = 24000\n",
        "duration_audio = 5\n",
        "seg_length = int(duration_audio * 48000)\n",
        "start_sample = np.random.randint(len(full_dry)-duration_audio*48000)\n",
        "dry = full_dry[start_sample:start_sample+seg_length]\n",
        "amped = full_amped[start_sample:start_sample+seg_length]\n",
        "with torch.no_grad():\n",
        "  modeled = model(dry[:, None, None].to(device)).cpu().flatten().detach().numpy()\n",
        "\n",
        "\n",
        "print(f\"Current model: {file_name}_{config_file}\")\n",
        "print(f\"ESR:\", esr)\n",
        "# Visualization\n",
        "fig = go.Figure()\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x=list(np.arange(len(dry[:samples_viz]))/48000), y=dry[:samples_viz],\n",
        "        name=\"dry\", mode='lines'\n",
        "    )\n",
        ")\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x=list(np.arange(len(amped[:samples_viz]))/48000), y=amped[:samples_viz],\n",
        "        name=\"target\", mode='lines'\n",
        "    )\n",
        ")\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x=list(np.arange(len(modeled[:samples_viz]))/48000), y=modeled[:samples_viz],\n",
        "        name=\"prediction\", mode='lines'\n",
        "    )\n",
        ")\n",
        "fig.update_layout(\n",
        "    title=\"Dry vs Target vs Predicted signal\",\n",
        "    xaxis_title=\"Time (s)\",\n",
        "    yaxis_title=\"Signal Amplitude\",\n",
        "    legend_title=\"Signal\",\n",
        ")\n",
        "fig.show()\n",
        "\n",
        "# Listen\n",
        "print(\"DRY Signal:\")\n",
        "IPython.display.display(IPython.display.Audio(data=dry, rate=48000))\n",
        "\n",
        "print(\"TARGET Signal:\")\n",
        "IPython.display.display(IPython.display.Audio(data=amped, rate=48000))\n",
        "\n",
        "print(\"PREDICTED Signal:\")\n",
        "IPython.display.display(IPython.display.Audio(data=modeled, rate=48000))\n",
        "\n",
        "print(\"Difference Signal:\")\n",
        "IPython.display.display(IPython.display.Audio(data=amped-modeled, rate=48000))\n",
        "\n",
        "step = max(step, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "B-9xp6fxmLmI"
      },
      "outputs": [],
      "source": [
        "#@markdown `<- RUN CELL (‚ñ∫)`\n",
        "\n",
        "#@markdown Here you can **upload** your own dry guitar files, and listen to the predicted output of the model.\n",
        "assert 'step' in locals(), \"Please run the code in the introduction section first!\"\n",
        "assert step>=1, \"Please execute the \\\"1.DATA\\\" cell code to prepare the data for the training!\"\n",
        "assert step>=2, \"Please execute the \\\"2.TRAINING\\\" cell code to train a model for evaluation!\"\n",
        "\n",
        "print(\"---\")\n",
        "\n",
        "uploaded = files.upload()\n",
        "print()\n",
        "print(\"Running predictions:\")\n",
        "\n",
        "def wav2tensor(filepath):\n",
        "  aud, sr = librosa.load(filepath, sr=None, mono=True)\n",
        "  aud = librosa.resample(aud, orig_sr=sr, target_sr=48000)\n",
        "  return torch.tensor(aud)\n",
        "\n",
        "\n",
        "for k, v in uploaded.items():\n",
        "  print(\"#####\", k)\n",
        "  dry = wav2tensor(io.BytesIO(v))\n",
        "  with torch.no_grad():\n",
        "    modeled = model(dry[:, None, None].to(device)).cpu().flatten().detach().numpy()\n",
        "\n",
        "  print(\"DRY Signal:\")\n",
        "  IPython.display.display(IPython.display.Audio(data=dry, rate=48000))\n",
        "\n",
        "  print(\"PREDICTED Signal:\")\n",
        "  IPython.display.display(IPython.display.Audio(data=modeled, rate=48000))\n",
        "\n",
        "  step = max(step, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjYqtq7FoK0Z"
      },
      "source": [
        "# 5. Model Export ‚úÖ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "aTXO1011OWVI"
      },
      "outputs": [],
      "source": [
        "#@markdown `<- RUN CELL (‚ñ∫)`\n",
        "\n",
        "#@markdown Download a json file summarizing the model that you just traineed.\n",
        "\n",
        "#@markdown You can then upload it to AIDA-X model loader plugin and run it in real-time.\n",
        "\n",
        "assert 'step' in locals(), \"Please run the code in the introduction section first!\"\n",
        "assert step>=1, \"Please execute the \\\"1.DATA\\\" cell code to prepare the data for the training!\"\n",
        "assert step>=2, \"Please execute the \\\"2.TRAINING\\\" cell code to train a model for evaluation!\"\n",
        "\n",
        "print(\"---\")\n",
        "model_filename = os.path.split(model_dir)[-1]+'.json'\n",
        "print(\"Generating model file:\", model_filename)\n",
        "\n",
        "model_path = model_dir+'/model_best.json'\n",
        "!python3 modelToKeras.py -lm \"$model_path\"\n",
        "\n",
        "shutil.copyfile(os.path.join(model_dir, 'model_keras.json'), os.path.join('/content', os.path.split(model_dir)[-1]+'.json'))\n",
        "files.download(os.path.join('/content', model_filename))\n",
        "\n",
        "print()\n",
        "print(\"Model file downloading...\")\n",
        "step = max(step, 4)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
